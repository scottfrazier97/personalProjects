{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "b1e01b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5250a7a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = load_iris()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "906681dd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'frame': None,\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': 'C:\\\\Users\\\\scott\\\\Anaconda3\\\\lib\\\\site-packages\\\\sklearn\\\\datasets\\\\data\\\\iris.csv'}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "eef84395",
   "metadata": {},
   "outputs": [],
   "source": [
    "iris1 = pd.DataFrame(df.data, columns = df.feature_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0ab36c8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "y = df.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "4b35cdbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestClassifier\n",
    "rfmodel = RandomForestClassifier(n_estimators=100, criterion='gini', max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "7d8933cd",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(max_depth=4)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel.fit(iris1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "4df87a25",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9866666666666667"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel.score(iris1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a648e35a",
   "metadata": {},
   "outputs": [],
   "source": [
    "rfmodel2 = RandomForestClassifier(n_estimators=10, criterion='gini', max_depth=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "b019d65d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.98"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfmodel2.fit(iris1, y)\n",
    "rfmodel2.score(iris1, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "4be1e5e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.96666667, 0.96666667, 0.93333333, 0.9       , 1.        ])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "cross_val_score(rfmodel2, iris1, y, cv=5, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "855ebe76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 [0.93333333 0.96666667 0.86666667 0.93333333 1.        ]\n",
      "5 [0.96666667 0.96666667 0.9        0.96666667 1.        ]\n",
      "10 [0.96666667 0.96666667 0.9        0.9        1.        ]\n",
      "20 [0.96666667 0.96666667 0.93333333 0.9        1.        ]\n",
      "50 [0.96666667 0.96666667 0.9        0.96666667 1.        ]\n",
      "100 [0.96666667 0.96666667 0.93333333 0.93333333 1.        ]\n",
      "200 [0.96666667 0.96666667 0.93333333 0.93333333 1.        ]\n",
      "300 [0.96666667 0.96666667 0.93333333 0.93333333 1.        ]\n"
     ]
    }
   ],
   "source": [
    "for i in [1,5,10,20,50,100,200,300]:\n",
    "    rfmodel2.n_estimators = i\n",
    "    print(i, cross_val_score(rfmodel2, iris1, y, cv=5, scoring='accuracy') )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ad9d969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efaa52ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3346bfa4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd274969",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a450ddc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f73b612a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2f16c588",
   "metadata": {},
   "source": [
    "Welcome to the software demonstration on random forest. Let's go ahead and get started, do our standard imports. All right, so you had a little problem with the iris. Let's deal with that in just a second and get our stuff in.\n",
    "\n",
    "All right, so what we're going to do is, we're going to use the iris dataset. It might be possibly capitalized. Let's give it a shot here. It's probably the load iris. There we go.\n",
    "\n",
    "All right, so once again, we'll get this array. And we'll have to take a look at the data types to get it. We see we've got a target. We've got the target names. All right, so we're going to have to do a little modification of that.\n",
    "\n",
    "And now, we've got down here the feature names. So these will be our column names. So we'll have to remember that as we create our DataFrame.\n",
    "\n",
    "So we'll just call this iris data. That may seem a little crazy to have data data. But remember, I put that in a data object.\n",
    "\n",
    "And as I scroll up here, what you may not realize is, there's a key in the dictionary called data. So for our DataFrame, that's what we're going to do. We'll need an actual DataFrame. So there we go.\n",
    "\n",
    "And of course, the columns are the feature names. So let's just double check and make sure we're calling that correctly. Yep, feature names, and we can even copy it directly out of there. Let's take a look.\n",
    "\n",
    "So this is our classic iris dataset. We'll also needs some targets here. We typically call that y. Let's take a look at what that's looking like.\n",
    "\n",
    "All right, so that's already performed as integers for us. So we've got a multiclass classification, something that's slightly complex. The iris dataset isn't too bad. But we've got a multiclass classification. So we want to take our first attempt at modeling this.\n",
    "\n",
    "So our first attempt is the random forest. Random forest is located in the ensemble part of the package. Now, you'll notice there's a couple of options. Since we're doing a classification, we'll obviously want the RandomForestClassifier. If we were doing the regressor, we would choose the latter.\n",
    "\n",
    "So let's start with the RandomForestClassifier. And of course, I like to do my model names by initials. But let's just call it our rf_model.\n",
    "\n",
    "Now, in the past couple of videos, I'd like to pull up the documentation just to refresh, because there is so much in this package. So let's go ahead and take a look at the random forest.\n",
    "\n",
    "We're doing the RandomForestClassifier. So we can see now we're dealing with a quite complex class that has a number of things that we need to control here at the beginning. One of the most important is the number of estimators.\n",
    "\n",
    "You'll notice the default is 100. So that means if you add no extra information and just use the default with the rounded braces, you'll get a random forest with 100 trees. That's the number of estimators.\n",
    "\n",
    "We see that the default criterion is gini. Now, we've studied both gini and entropy. And so we can switch back and forth between them. We'll just keep in the back of our mind that the default is using gini for information gain.\n",
    "\n",
    "Now we start to see all the things that we control for our partition trees. And of course, each tree will be subject to that. And that includes the max depth, the number of samples required for a split, the number of samples for a leaf, and of course, things like weight fractions and the number of features.\n",
    "\n",
    "Now, you'll notice these limits are set quite low. And of course, for a random forest, we're not concerned with the individual tree being overfit. In fact, we don't care if they are greatly overfit. Because what's important for us is the overall ensemble. And the overfit trees selected from random data will actually help us by being less correlated to each other.\n",
    "\n",
    "That may seem counterintuitive, but it's actually one of the powers of the random classifier. So we've seen kind of what we need to do. So let's make some initial guesses on what our classifier might do.\n",
    "\n",
    "We've seen that the number of estimators is default 100, but let's just put that in. And let's put in our criteria. Now, let's see if what happens-- we'll start with a max depth. And let's just try something like 4.\n",
    "\n",
    "And again, these are guesses. And we're going to have to tune all of these. But we know that from our work on partition trees, that the iris data set doesn't have a lot. So we probably don't want to do too much depth.\n",
    "\n",
    "So we've got kind of our initial setup, our initial guess. We've put in some parameters, not all. And again, each time you do this, you're going to have to make a decision on what parameters you want to check. And we'll start to see that each of these takes more and more time to fit.\n",
    "\n",
    "So we've got all this set up. We're going to create our model. Now it's time to go ahead and fit our model. Let's remember what we actually called it, our iris_data.\n",
    "\n",
    "So that didn't actually take too long. And let's see what we have when we fit this model. So you can see we've got a number of issues, or not issues, but a number of things we can take a look at.\n",
    "\n",
    "We see the estimator parameters. We can see the individual trees. You'll notice we have feature importances.\n",
    "\n",
    "We may not have talked about feature importances, but it's handy to know in random forests. And it turns out the feature importance is related to how many times that feature was used to make a split. So this can be handy for helping us to interpret these forest classifiers.\n",
    "\n",
    "So while we often rely on linear models for feature importance, remember, you can do it for random forests too, it's just less clear. Because it's not about the relationship to the output. It's more about how often that parameter was used to make a split.\n",
    "\n",
    "We can see all our different variables. And what we're looking for is the score. Now, you may wonder, what is this oob_score? What this is going to be is our out-of-bagging score.\n",
    "\n",
    "Because random forest takes a bagging approach, what that means is the samples that were not used to build the trees were actually run through as a pseudo test set. So this can be used as a replacement for your basically out-of-fold predictions. Although, because there is some bias involved, it's not always the best choice.\n",
    "\n",
    "We're still going to go with our cross-validation. But be aware that this is available for your random forest. What we're really looking for is the score.\n",
    "\n",
    "Let's take a look at that score. And we'll need to score our data. And so, we see this is actually an accuracy score, and we've got a fairly accurate model. So let's see what can happen as we start to change some of these parameters.\n",
    "\n",
    "So instead of having 100 estimators, let's just crank this down to 10. So I'm just going to redo my model, set the estimators to 10. Let's fit the model again. And let's take a look at the score.\n",
    "\n",
    "Now again, this is the biased score. But we're just looking at what happens as we vary the model parameters. You'll notice we still get a pretty high score.\n",
    "\n",
    "But notice something key. We're losing some precision. Not precision of the model, but we're down to getting scores that, instead of out to multiple decimal places, come down to a single decimal place. In fact, if we start doing estimates, you'll see that our scores will become 90, 80, 70, as we crank these up and down. Because the score is actually related.\n",
    "\n",
    "If we looked at accuracy in particular, it is related to the vote. And because there are 10 estimators in our vote, we would have that as our score. Let's see if we can approximate that with our cross-validation.\n",
    "\n",
    "For a reminder, we always do cross-validation because we don't want to get biased estimates of our score. And so we need the out-of-fold predictions. Let's just be sure we're using five folds. And let's see if we can get our accuracy in here.\n",
    "\n",
    "So you can see, what we're getting is some of the folds are extremely accurate, while some of them vary from 96% down to 93%. Let's see what happens as we start to change some of the parameters. So let's see if we can set the number of estimators in our random forest model. So it looks like we can. So let's see if we can use our previous method of iterating through and watching how the scores change.\n",
    "\n",
    "I'm going to do something a little bit different this time. I'm going to provide a list. So we're just going to select each item out of that list and see what happens to the score. Close up our parentheses and watch what happens.\n",
    "\n",
    "Now, you'll notice that the scores didn't change that much. So what's happening in the random forest, and is unique about the random forests, is we don't suffer so much from overfitting. We may have to tune to get the best model. But even as we crank up the number of estimators, the overfitting really doesn't become a problem.\n",
    "\n",
    "You may be concerned about this last value dropping from 96 to 93. It's actually because the data set is so small, that represents a single data point. So while that may be an issue for us, in general, as you crank up the number of estimators in a random forest, you really don't suffer much.\n",
    "\n",
    "So it's to your advantage to use the largest number of estimators that you can. This is one of the unique features of random forests. However, it doesn't apply to any of the other parameters, only to the number of estimators.\n",
    "\n",
    "Now, if you review all the things that we had, we could go through this loop multiple times. We could be here all day trying to vary these parameters. And in future lectures, we'll talk about how to properly tune various models with multiple parameters.\n",
    "\n",
    "So we won't do that here. But hopefully this starts to give you an idea of what to do for the random forest. But let's take a look at what happens when we vary some other parameters, just to get an understanding for the random forest and become comfortable with using it.\n",
    "\n",
    "Let's see what happens when we switch to the entropy loss or the entropy information gain. So we're going to create a new model. I'll just copy this down here. We'll maybe denote it by entropy. And we'll switch this, go back to our default of 100.\n",
    "\n",
    "And let's see what happens when we fit this model. Let's actually do a cross-validation score to compare. You can see there really isn't much change between entropy and gini.\n",
    "\n",
    "So this is a key takeaway. That while there might be a specific problem with entropy or gini, it really doesn't matter which one you use. And it's really important to figure out which one gives you the better result. Once again, there's no hard-and-fast rule.\n",
    "\n",
    "Some of the other things we want to take a look at is the maximum depth. But before we do that, let's take a look at the documentation to see if maybe there's some guidance about this. So if we look at the max depth here, we see it's the max depth of the tree. And if we leave it blank, it means everything is expanded until all the other criteria are met. That's essentially what this is saying.\n",
    "\n",
    "Now, we intentionally limited this to 4. So let's see what happens when we turn it off. There's one other thing I want to take a look at while we're here. And that's the max features. Now, we didn't input anything for max features. And you'll notice there's various options here. And we get a little blurb about each one.\n",
    "\n",
    "So if we actually provide an integer, it's going to use that integer features for split. So if I say the max features is 10, what the random forest is going to do is it's going to look at 10 features to consider the split. If we provide a float, that will be a percentage of the features. If we provide auto, that means we're going to use the square root of the number of features.\n",
    "\n",
    "This is kind of your handy rule of thumb for forests for how many max features to use. In general, it should be the square root of N. But again, each problem is different. So this is something that we can experiment.\n",
    "\n",
    "Again, we may not go through all the iterations at this point. But I'm trying to highlight the important ones that you can spend your time on that are going to give you the most benefit rather than playing with every single one and trying to find the max improvement. So in general, what you're going to want to look for is the max features, the max depth, and possibly switching between your different criteria. Again, the estimators, you really don't have to play with that. You set it to a large number, really as large as you can tolerate as terms of time.\n",
    "\n",
    "Hopefully, you saw in the video that as we went to larger and larger numbers, it does slow down. But the other important thing is, there is hidden in here a number of jobs. So if you have a multiprocessor computer, and most people do, don't forget to take advantage of this to allow your multiprocessing to help speed up your job. And again, that allows you to get a little bit more accuracy.\n",
    "\n",
    "So let's take a look and just do how things change as we swap a couple of these things. So we'll keep our entropy model. But now, instead of the max depth 4, we'll just eliminate it, and it'll be as many features as we want. And let's see if we get much of a difference.\n",
    "\n",
    "So you can see we actually get a little bit of an improvement on this. It's not significant, because we were already pretty high. But this shows you that these parameters do have an impact on your final results.\n",
    "\n",
    "Now, the iris data set is a fairly simple data set. So it's pretty easy to get good results. But it's also a good way for you to learn your new classifiers.\n",
    "\n",
    "And remember, we didn't do anything with multiclass. The random forest was able to handle it automatically. That's why I like to use this as my first choice when I'm taking an attempt to make a new model.\n",
    "\n",
    "The random forest is extremely user friendly, is able to handle multiple classes, and is pretty easy to optimize. We usually, once I've got a good score out of my random forest, then I try and beat it with my more advanced models. That tells me that I'm improving on my score.\n",
    "\n",
    "If I can't beat the random forest score, that tells me that something's wrong with my advanced models. Random forest kind of stands as our baseline for what we should be able to achieve. Keep that in mind as you go through when you're tuning these models.\n",
    "\n",
    "This concludes our video on the random forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0cb0b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
